# ðŸ“¡ Realtime CDC Data Pipeline for Recruitment Start-up
ðŸ—ï¸ Giá»›i thiá»‡u dá»± Ã¡n

Dá»± Ã¡n nÃ y mÃ´ phá»ng má»™t Realtime Change Data Capture (CDC) Pipeline hoÃ n chá»‰nh, tÆ°Æ¡ng tá»± há»‡ thá»‘ng dá»¯ liá»‡u cá»§a má»™t startup tuyá»ƒn dá»¥ng.
Hai tá»‡p dá»¯ liá»‡u tracking.csv vÃ  search.csv Ä‘Ã³ng vai trÃ² nguá»“n sá»± kiá»‡n giáº£ láº­p tá»« client. CÃ¡c sá»± kiá»‡n nÃ y Ä‘Æ°á»£c náº¡p vÃ o PostgreSQL, sau Ä‘Ã³ toÃ n bá»™ thay Ä‘á»•i Ä‘Æ°á»£c Debezium CDC báº¯t láº¡i vÃ  Ä‘áº©y vÃ o Kafka dÆ°á»›i dáº¡ng stream.

Spark Streaming tiáº¿p nháº­n dá»¯ liá»‡u tá»« Kafka, thá»±c hiá»‡n bÆ°á»›c transform & enrich, rá»“i ghi káº¿t quáº£ Ä‘Ã£ xá»­ lÃ½ vÃ o MySQL.
Cuá»‘i cÃ¹ng, Grafana sá»­ dá»¥ng MySQL lÃ m datasource Ä‘á»ƒ trá»±c quan hÃ³a dá»¯ liá»‡u theo thá»i gian thá»±c (real-time dashboards).

![Capture.PNG](images%2FCapture.PNG)
## ðŸ“ Project Structure
```
realtime-cdc-pipeline-prj2/
â”‚
â”œâ”€â”€ checkpoint/
â”‚   â”œâ”€â”€ search_offset.txt
â”‚   â””â”€â”€ tracking_offset.txt
â”‚
â”œâ”€â”€ csv_files/
â”‚   â”œâ”€â”€ search.csv
â”‚   â”œâ”€â”€ tracking.csv
â”‚   â””â”€â”€ tracking_clean.csv
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ img.png
â”‚
â”œâ”€â”€ producers/
â”‚   â”œâ”€â”€ load_search_to_postgres.py
â”‚   â””â”€â”€ load_tracking_to_postgres.py
â”‚
â”œâ”€â”€ set-up-mysql-db/
â”‚   â””â”€â”€ mysql_schema.sql
â”‚
â”œâ”€â”€ set-up-postgres-db/
â”‚   â””â”€â”€ pg_schema.sql
â”‚
â”œâ”€â”€ streams/
â”‚   â”œâ”€â”€ search_stream.py
â”‚   â””â”€â”€ tracking_stream.py
â”‚
â”œâ”€â”€ venv/
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ clean_csv.py
â”œâ”€â”€ debezium-config.sh
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ main.py
â”œâ”€â”€ mysql-connector-j-8.1.0.jar
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ test.py
â”œâ”€â”€ tracking_connector.json
â””â”€â”€ README.md

```
### ðŸ§© 1. Kiáº¿n trÃºc tá»•ng quan

Pipeline:

CSV â†’ Producers â†’ PostgreSQL

Debezium CDC theo dÃµi má»i thay Ä‘á»•i â†’ Ä‘áº©y vÃ o Kafka

Kafka chá»©a cÃ¡c CDC topic

Spark Streaming Ä‘á»c Kafka â†’ transform â†’ ghi vÃ o MySQL

Grafana Ä‘á»c MySQL â†’ dashboard realtime

### ðŸ³ 2. Khá»Ÿi cháº¡y toÃ n bá»™ services
Trong root project:

```commandline
docker-compose up -d
```


Kiá»ƒm tra container:

```commandline
docker ps
```


Báº¡n sáº½ tháº¥y:

![img_1.png](images%2Fimg_1.png)

### ðŸ—„ï¸ 3. Thiáº¿t láº­p PostgreSQL
#### ðŸ§‚ BÆ°á»›c 1 â€” Truy cáº­p container postgres
```commandline
docker exec -it postgres psql -U postgres
```
Check database:
```
\l
```
ðŸ‘‰ Náº¿u khÃ´ng cÃ³ etl_db, táº¡o:

```
CREATE DATABASE etl_db;
```
#### ðŸ§‚ BÆ°á»›c 2 â€” Load schema

Truy cáº­p database:

```commandline
docker exec -it postgres psql -U postgres -d etl_db
```

Load file SQL:

```commandline
     i /tmp/pg_schema.sql
```

Náº¿u file chÆ°a tá»“n táº¡i trong container â†’ copy:
```commandline
docker cp set-up-postgres-db/pg_schema.sql postgres:/tmp/pg_schema.sql
```
screenshot psql tables
![img_5.PNG](images%2Fimg_5.PNG)

### ðŸ” 4. Insert CSV vÃ o PostgreSQL
```commandline
def load_search_csv(file_path, offset_file):
    df = pd.read_csv(file_path, header=0)
    # loáº¡i bá» cá»™t index thá»«a vÃ  reset index
    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    df.reset_index(drop=True, inplace=True)
    columns = df.columns.tolist()

    last_offset = read_offset(offset_file)
    new_rows = df.iloc[last_offset:]
    print(f"[SEARCH] Inserting {len(new_rows)} new rows starting from offset {last_offset}...")

    for idx, row in new_rows.iterrows():
        values = [none_if_nan(row[col]) for col in columns]
        values.append(datetime.now())  # updated_at timestamp

        placeholders = ','.join(['%s'] * len(values))
        sql = f"""
        INSERT INTO search_by_jobid ({','.join(columns)}, updated_at)
        VALUES ({placeholders})
        ON CONFLICT (job_id) DO NOTHING
        """
        cursor.execute(sql, tuple(values))

        print(f"[SEARCH] Inserted row {idx + 1}: job_id={row['job_id']}")
        write_offset(offset_file, idx + 1)  # checkpoint = index + 1

        time.sleep(0.2)  # delay giáº£ láº­p realtime
```

Trong local terminal:
```commandline
python producers/load_search_to_postgres.py
python producers/load_tracking_to_postgres.py
```
![img_3.png](images%2Fimg_3.png)
### ðŸ‘¤ 5. Táº¡o user replication cho Debezium
Trong Postgres:
```commandline
docker exec -it postgres psql -U postgres -d etl_db
```
Run:
```commandline
CREATE ROLE debezium WITH LOGIN PASSWORD 'debezium';
ALTER ROLE debezium REPLICATION;
GRANT CONNECT ON DATABASE etl_db TO debezium;
```
### ðŸ“£ 6. Táº¡o Publication Ä‘á»ƒ Debezium theo dÃµi báº£ng
```commandline
CREATE PUBLICATION dbz_pub FOR ALL TABLES;
```
### ðŸ”Œ 7. Táº¡o Debezium Connector
Cáº¥u hÃ¬nh debezium:
```commandline
{
  "name": "cdc_tracking_conn",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "tasks.max": "1",
    "plugin.name": "pgoutput",
    "database.hostname": "postgres",
    "database.port": "5432",
    "database.user": "debezium",
    "database.password": "debezium",
    "database.dbname": "etl_db",
    "database.server.name": "cdc",
    "publication.name": "dbz_pub",
    "slot.name": "cdc_slot",
    "table.include.list": "public.tracking_events,public.search_by_jobid",
    "topic.prefix": "cdc",
    "tombstones.on.delete": "false",
    "decimal.handling.mode": "string",
    "snapshot.mode": "initial"
  }
}

```

```commandline
$body = Get-Content .\tracking_connector.json -Raw
Invoke-RestMethod -Uri http://localhost:8083/connectors `
  -Method Post -ContentType "application/json" -Body $body
```
![img_2.png](images%2Fimg_2.png)

### ðŸ“¡ 8. Kiá»ƒm tra Kafka Topic CDC
VÃ o Kafka container:

```
docker exec -it kafka bash
```

List topic:
```
kafka-topics --bootstrap-server kafka:29092 --list
```

Báº¡n sáº½ tháº¥y:
```
cdc.public.tracking_events
cdc.public.search_by_jobid
```

Äá»c realtime:
```
kafka-console-consumer --bootstrap-server kafka:29092 \
  --topic cdc.public.tracking_events --from-beginning
```
```
kafka-console-consumer --bootstrap-server kafka:29092 \
  --topic cdc.public.search_by_jobid --from-beginning
```
![img_4.png](images%2Fimg_4.png)

### ðŸ”¥ 9. Cháº¡y Spark Streaming
```commandline
# Khá»Ÿi táº¡o SparkSession
spark = SparkSession.builder \
    .appName("SearchByJobStream") \
    .config("spark.jars", "file:///E:/data-projects/realtime-cdc-pipeline-prj2/mysql-connector-j-8.1.0.jar") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .getOrCreate()

# Schema cho pháº§n payload.after
schema = StructType([
    StructField("job_id", StringType()),
    StructField("company_name", StringType()),
    StructField("title", StringType()),
    StructField("city_name", StringType()),
    StructField("state", StringType()),
    StructField("major_category", StringType()),
    StructField("minor_category", StringType()),
    StructField("pay_from", StringType()),
    StructField("pay_to", StringType()),
    StructField("pay_type", StringType()),
    StructField("work_schedule", StringType())
    # updated_at bá» qua, MySQL tá»± fill
])

# Äá»c Kafka
df = (spark.readStream.format("kafka")
      .option("kafka.bootstrap.servers", "localhost:9092")
      .option("subscribe", "cdc.public.search_by_jobid")
      .option("startingOffsets", "earliest")
      .load())

# Parse JSON: láº¥y payload.after
df_parsed = df.selectExpr("CAST(value AS STRING) as json_str") \
    .select(get_json_object(col("json_str"), "$.payload.after").alias("after_json")) \
    .select(from_json(col("after_json"), schema).alias("data")) \
    .select("data.*") \
    .withColumn("pay_from", col("pay_from").cast("float")) \
    .withColumn("pay_to", col("pay_to").cast("float")) \
    .filter(col("job_id").isNotNull())
```
Trong terminal:

##### Tracking stream
```
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
  streams/tracking_stream.py
```
##### Search stream
```
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
  streams/search_stream.py
```
Spark logs 
![img_6.PNG](images%2Fimg_6.PNG)

### ðŸ—„ï¸ 10. MySQL (Storage sau transform)

```commandline
def write_to_mysql(batch_df, epoch_id):
    try:
        count = batch_df.count()
        logging.info(f"[Batch {epoch_id}] Rows to write: {count}")
        if count > 0:
            batch_df.show(truncate=False)   # xem dá»¯ liá»‡u thá»±c táº¿
            batch_df.printSchema()          # xem schema
            batch_df.write \
                .format("jdbc") \
                .mode("append") \
                .option("url", "jdbc:mysql://localhost:3306/etl_db?useSSL=false&serverTimezone=UTC") \
                .option("driver", "com.mysql.cj.jdbc.Driver") \
                .option("dbtable", "search_by_jobid") \
                .option("user", "root") \
                .option("password", "123456") \
                .save()
            logging.info(f"[Batch {epoch_id}] Written {count} rows to MySQL")
    except Exception as e:
        logging.error(f"[Batch {epoch_id}] Error writing to MySQL: {e}")
```
![img_7.PNG](images%2Fimg_7.PNG)

### ðŸ“Š 11. Grafana Visualization
1. Má»Ÿ Grafana:
http://localhost:3000

Login: admin / admin

2. Add data source â†’ MySQL

3. Viáº¿t query vÃ­ dá»¥:
```
SELECT minor_category, COUNT(*) AS job_count
FROM search_by_jobid
GROUP BY minor_category
ORDER BY job_count DESC;

```
![img_8.png](images%2Fimg_8.png)